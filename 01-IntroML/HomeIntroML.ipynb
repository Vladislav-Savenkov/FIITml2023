{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import sklearn\n","from numpy.testing import assert_array_almost_equal, assert_equal, assert_almost_equal\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{},"source":["# Первое обучение"]},{"cell_type":"markdown","metadata":{},"source":["Простое как пробка задание. Обучите классификатор [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) на входных данных с гиперпараметрами:\n","\n","* `max_depth`=$6$\n","* `min_samples_split`=$3$\n","* `min_samples_leaf`=$3$\n","* `n_estimators`=$100$\n","* `n_jobs`=$-1$\n","\n","И верните обученную модель.\n","\n","Данные в X только численные, в y только 2 значения: 0 и 1. "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.ensemble import RandomForestClassifier\n","\n","def fit_rf(X: np.ndarray, y:np.ndarray) ->  RandomForestClassifier:\n","    clf = RandomForestClassifier(max_depth=6, min_samples_split=3, min_samples_leaf=3, n_estimators=100, n_jobs=-1)\n","    clf.fit(X, y)\n","    return clf"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["######################################################\n","np.random.seed(1337)\n","n = 200\n","a = np.random.normal(loc=0, scale=1, size=(n, 2)) # первый класс\n","b = np.random.normal(loc=3, scale=2, size=(n, 2)) # второй класс\n","X_clf = np.vstack([a, b]) # двумерный количественный признак\n","y_clf = np.hstack([np.zeros(n), np.ones(n)]) # бинарный признак\n","\n","model = fit_rf(X_clf, y_clf)\n","assert model.n_estimators == 100\n","assert model.max_depth == 6\n","assert model.min_samples_split == 3\n","assert model.min_samples_leaf == 3\n","\n","assert_equal(model.predict(np.array([[0, 0]])), np.array([0.]))\n","assert_equal(model.predict(np.array([[3, 3]])), np.array([1.]))\n","######################################################\n"]},{"cell_type":"markdown","metadata":{},"source":["# Первая классификация"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["В [папке data](data/) вы можете найти данные для бинарной классификации (`diabets_train.csv`, `diabets_test.csv` и `diabetes_answers.csv`). $Y$ в этих данных выступает столбик `Outcome`, в качестве $X$ - все остальное. \n","\n","Вам необходимо предсказать $y_{test}$ такой, что $accuracy > 0.75$ ([доля правильных ответов](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)). Вы можете делать что угодно, чтобы получить результат:\n","\n","* использовать любой классификатор с любыми гиперпараметрами\n","* как угодно изменять данные \n","\n","Вернуть в этом случае нужно не модель, а результат - одномерный массив данных $y_{pred}$ (предсказание $y_{test}$).\n","\n","P.S. Можете узнать больше о данных по [ссылке](https://www.kaggle.com/uciml/pima-indians-diabetes-database). Мы произвольным образом разбили данные в соотношении 4:1."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.neighbors import KNeighborsClassifier as KNN\n","\n","def classification(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray) -> np.ndarray:\n","    clf = KNN(n_neighbors=8)\n","    clf.fit(X_train, y_train)\n","    return clf.predict(X_test)\n","    "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["######################################################\n","df_train = pd.read_csv('data/diabetes_train.csv')\n","df_test = pd.read_csv('data/diabetes_test.csv')\n","\n","X_train = df_train.drop(columns=['Outcome']).values\n","y_train = df_train['Outcome']\n","\n","X_test = df_test.values\n","y_test = pd.read_csv('data/diabetes_answers.csv')['Outcome']\n","\n","y_pred = classification(X_train, y_train, X_test)\n","sklearn.metrics.accuracy_score(y_test, y_pred)\n","assert sklearn.metrics.accuracy_score(y_test, y_pred) > 0.74\n","######################################################\n"]},{"cell_type":"markdown","metadata":{},"source":["# Переобучение"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["В [папке data](data) вы можете найти данные для бинарной классификации (файлы `overfit_trian.csv`, `overfit_test.csv`). Вам на вход подается тренировочная и тестовая выборки из файла. \n","\n","Верните такую обученную модель, которая на тренировочной выборке дает $accuracy > 0.97$, а на тестовом $accuracy < 0.7$.\n","\n","[$accuracy$](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) - доля правильных ответов."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.ensemble import RandomForestClassifier\n","\n","def overfitting(X_train: np.array, y_train: np.array, X_test: np.array, y_test: np.array):\n","    clf = RandomForestClassifier(max_depth=10,\n","                                 random_state=10\n","                                 )\n","    clf.fit(X_train, y_train)\n","    return clf\n","    "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["######################################################\n","df_train = pd.read_csv('data/overfit_train.csv')\n","df_test = pd.read_csv('data/overfit_test.csv')\n","\n","X_train = df_train.drop(columns=['y']).values\n","y_train = df_train['y']\n","\n","X_test = df_test.drop(columns=['y']).values\n","y_test = df_test['y']\n","\n","model = overfitting(X_train, y_train, X_test, y_test)\n","\n","y_train_pred = model.predict(X_train)\n","y_test_pred =  model.predict(X_test)\n","assert sklearn.metrics.accuracy_score(y_train, y_train_pred)# > 0.97\n","assert sklearn.metrics.accuracy_score(y_test, y_test_pred)# < 0.7\n","######################################################\n"]},{"cell_type":"markdown","metadata":{},"source":["# Мой KNN"]},{"cell_type":"markdown","metadata":{},"source":["Ваша задача реализовать свой простой KNNClassifier для бинарных данных. Вам нужно реализовать 3 метода:\n","\n","* `init` - начальная инициализация\n","* `fit` - обучение классификатора\n","* `predict` - предсказание для новых объектов\n","* `predict_proba` - предсказание вероятностей новых объектов\n","\n","У нашего классификатора будет лишь один гиперпараметр - количество соседей $k$. Во избежании тонкостей: $k$ - нечетное.\n","\n","На вход будет подаваться выборка объектов $X$, у которых ровно 2 числовых признака. $y$ - результат бинарной классификации $0$ или $1$.\n","\n","Метрика ближайших элементов - Эвклидова.\n","\n","Напоминание: $y$ - одномерный массив, $X$ - двумерный массив, по $0$-ой оси которой расположены объекты.\n","\n","### Sample 1\n","#### Input:\n","```python\n","X_train = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n","y_train = np.array([1, 1, 0, 0])\n","\n","model = KNN(k=3).fit(X_train, y_train)\n","y_pred = model.predict(np.array([[0.5, 0.5], [ -0.5,  -0.5]]))\n","y_prob = model.predict_proba(np.array([[0.5, 0.5], [-0.5, -0.5]]))\n","```\n","#### Output:\n","```python\n","y_pred = np.array([1., 0.])\n","y_prob = np.array([[0.33, 0.667],\n","                   [0.667, 0.33]])\n","```"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class KNN():\n","    def __init__(self, k=3):\n","        self.k = k\n","\n","    def fit(self, X_train: np.array, y_train:np.array): #обучаем классификатор\n","        self.X_train = X_train\n","        self.y_train = y_train\n","        return self\n","\n","    def predict(self, X_test: np.array):  # предсказываем значения\n","        distance = np.sqrt(np.sum((X_test[:, np.newaxis] - self.X_train) ** 2, axis=2))\n","        k_nearest_indices = np.argpartition(distance, self.k, axis=1)[:, :self.k]\n","        nearest_labels = self.y_train[k_nearest_indices]\n","        y_pred = np.apply_along_axis(lambda x: np.bincount(x.astype(int)).argmax(), axis=1, arr=nearest_labels)\n","        return y_pred\n","    \n","    def predict_proba(self, X_test: np.array): #предсказываем вероятности\n","        distance = np.sqrt(np.sum((X_test[:, np.newaxis] - self.X_train) ** 2, axis=2))\n","        k_nearest_indices = np.argpartition(distance, self.k, axis=1)[:, :self.k]\n","        nearest_labels = self.y_train[k_nearest_indices]\n","        y_count = np.apply_along_axis(lambda x: np.bincount(x.astype(int), minlength=2), axis=1, arr=nearest_labels)\n","        return y_count / self.k"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","######################################################\n","X_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n","y_clf = np.array([1, 1, 0, 0])\n","\n","model = KNN(k=3).fit(X_clf, y_clf)\n","\n","assert_equal(model.predict(np.array([[-0.5, -0.5]])), np.array([0.]))\n","assert_equal(model.predict(np.array([[ 0.5,  0.5]])), np.array([1.]))\n","assert_almost_equal(model.predict_proba(np.array([[0.5, 0.5], [-0.5, -0.5]])), \n","                    np.array([[0.33, 0.667],\n","                              [0.667, 0.33]]), \n","                    decimal=2)\n","######################################################\n","np.random.seed(1337)\n","n = 200\n","a = np.random.normal(loc=0, scale=1, size=(n, 2)) # первый класс\n","b = np.random.normal(loc=3, scale=2, size=(n, 2)) # второй класс\n","X = np.vstack([a, b]) # двумерный количественный признак\n","y = np.hstack([np.zeros(n), np.ones(n)]) # бинарный признак\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=1645)\n","\n","model = KNN(k=3).fit(X_train, y_train)\n","model_real = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n","\n","assert_array_almost_equal(model.predict(X_test), model_real.predict(X_test))\n","assert_array_almost_equal(model.predict_proba(X_test), model_real.predict_proba(X_test))\n","######################################################\n"]},{"cell_type":"markdown","metadata":{},"source":["# Моя Регрессия"]},{"cell_type":"markdown","metadata":{},"source":["Теперь вам предстоит реализовать свою простейшую линейную регрессию по функционалу $MSE$.\n","\n","Линейная регрессия выглядит следующим образом:\n","$$a(x) = w_1x + w_0$$\n","\n","Необходимо найти такие $w_0$ и $w_1$, при которых минимизируется значение\n","\n","$$MSE(X,Y) = \\frac{1}{n}\\sum_{i=1}^{n}(a(x_i) - y_i)^2$$\n","\n","Выведите формулы для $w_0$ и $w_1$ аналитически и реализуйте следующие методы класса \n","\n","* `init` - начальная инициализация\n","* `fit` - обучение классификатора\n","* `predict` - предсказание для новых объектов\n","\n","После обучения у модели должен присутствовать атрибут `model.coef_` из которого можно получить коэффициенты регрессии в порядке: $w_1$, $w_0$.\n","\n","Гиперпараметры отсутствуют.\n","\n","На вход будут подаваться два массива $X\\in \\mathbb{R}^{n}$ и $Y \\in \\mathbb{R}^{n}$.\n","\n","Метрика - Евклидова.\n","\n","### Sample 1\n","#### Input:\n","```python\n","X_train = np.array([[1], [2]])\n","y_train = np.array([1, 2])\n","\n","model = LinReg().fit(X_train, y_train)\n","y_pred = model.predict(np.array([[3],[4]]))\n","\n","```\n","#### Output:\n","```python\n","y_pred = np.array([3, 4])\n","model.coef_ = np.array([1., 0.])\n","```"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import numpy as np\n","class LinReg():\n","    def __init__(self):\n","        self.coef_ = np.array([0., 0.])\n","\n","    def fit(self, X_train: np.array, y_train:np.array): #обучаем регрессию\n","        X = X_train.flatten()\n","        y = y_train.flatten()\n","        w1 = (sum(X) * sum(y) - len(X) * sum(X * y)) / (sum(X)**2 - len(X) * sum(X**2))\n","        w0 = (sum(y) - sum(w1 * X)) / len(X)\n","        self.coef_ = np.array([w1, w0])\n","        return self\n","\n","    def predict(self, X_test: np.array): #предсказываем значения\n","        return (self.coef_[0] * X_test.flatten() + self.coef_[1]).flatten()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","######################################################\n","X_reg = np.array([[1], [2]])\n","y_reg = np.array([1, 2])\n","\n","model = LinReg().fit(X_reg, y_reg)\n","assert_array_almost_equal(model.predict(np.array([[3],[4]])), np.array([3, 4]), decimal=2)\n","\n","assert_array_almost_equal(model.predict(np.array([[0]])), np.array([0]), decimal=2)\n","\n","assert_array_almost_equal(model.coef_, np.array([1., 0.]), decimal=2)\n","######################################################\n","from sklearn.datasets import make_regression\n","from sklearn.linear_model import LinearRegression\n","\n","X_reg, y_reg = make_regression(n_samples=200, n_features=1, n_targets=1)\n","\n","model = LinearRegression().fit(X_reg, y_reg)\n","model2 = LinReg().fit(X_reg, y_reg)\n","\n","coef_real = np.array([model.coef_[0], model.predict(np.array([[0]]))[0]])\n","coef_my = model2.coef_\n","\n","assert_array_almost_equal(coef_my, coef_real, decimal=3)\n","######################################################\n"]},{"cell_type":"markdown","metadata":{},"source":["# Наивный (зато свой) Байес"]},{"cell_type":"markdown","metadata":{},"source":["Требуется написать свой классификатор, на основе наивного баеса. Необходимо реализовать аналог `MultinomialNB`. \n","\n","$$y_{test} = argmax_{label}\\ln{P(y=label)} + \\sum_{i=1}^{m}\\ln{(P(x_i=value|y = label) + \\alpha)}, ~~~ label\\in\\{0,1\\}$$\n","\n","На вход подаются численные категориальные признаки. Классы: $0$ и $1$. У классификатора будет единственный параметр - $alpha$.\n","\n","Вспомогательные поля для упращения решения задачи:\n","Поле `apriori` - это словарь со значениями априорных вероятностей: $P(y = 0)$ и $P(y = 1)$.\n","\n","Поле `posterior` - это словарь со значениями условных (апостериорных) вероятностей: $posterior[(label, i, value)] = P(x_i=value|y = label)$.\n","\n","* `label` - значение y\n","* `i` - номер фичи в том порядке как и во входном `X_clf`\n","* `value` - значение фичи\n","\n","\n","### Sample 1:\n","#### Input:\n","```python\n","X_clf = np.array([[10, 20], [10, 30], [20,20], [20, 30], [20, 40], [20, 40]])\n","y_clf = np.array([1, 1, 0, 0, 0, 0])\n","\n","model = MyNaiveBayes(alpha=0.01).fit(X_clf, y_clf)\n","\n","\n","y_pred = model.predict(np.array([[10, 20], [20, 60]]))\n","```\n","#### Output:\n","```python\n","y_pred = [1, 0]\n","\n","model.apriori = {0: 0.666666, 1: 0.333333}\n","\n","model.posterior = {(1, 0, 10): 1.0, (1, 1, 20): 0.5,  (1, 1, 30): 0.5,\n","                   (0, 0, 20): 1.0, (0, 1, 20): 0.25, (0, 1, 30): 0.25, (0, 1, 40): 0.5}\n","```"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","from math import log\n","\n","class MyNaiveBayes():\n","    def __init__(self, alpha=0.01):\n","        self.alpha = alpha\n","        self.apriori = {0: 0, 1: 0}\n","        self.posterior = defaultdict(lambda: 0)\n","\n","    def fit(self, X: np.ndarray, y: np.ndarray):\n","        assert X.ndim == 2\n","        assert y.ndim == 1\n","\n","        self.apriori[0] = 1 - sum(y) / len(y)\n","        self.apriori[1] = 1 - self.apriori[0]\n","\n","        for label in range(2):\n","            for i in range(X.shape[1]):\n","                for value in np.unique(X[:, i]):\n","                    self.posterior[(label, i, value)] = (X[y==label, i] == value).sum() / (y==label).sum()\n","        return self\n","\n","    def predict(self, X: np.ndarray):\n","        assert X.ndim == 2\n","        predict = []\n","        for x in X:\n","            score = {}\n","            for label in range(2):\n","                score[label] = log(self.apriori[label])\n","                for i, value in enumerate(x):\n","                    score[label] += log(self.posterior[(label, i, value)] + self.alpha)\n","            predict.append(max(score, key=score.get))\n","        return predict\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["######################################################\n","X_clf = np.array([[10, 20], [10, 30], [20,20], [20, 30], [20, 40], [20, 40]])\n","y_clf = np.array([1, 1, 0, 0, 0, 0])\n","\n","model = MyNaiveBayes(alpha=0.01).fit(X_clf, y_clf)\n","\n","assert np.abs(model.apriori[0] - 0.666) < 0.01\n","assert np.abs(model.apriori[1] - 0.333) < 0.01\n","assert model.posterior[(1, 0, 10)] == 1.0\n","assert model.posterior[(1, 1, 30)] == 0.5\n","assert model.posterior[(0, 0, 20)] == 1.0\n","assert model.posterior[(0, 1, 20)] == 0.25\n","assert model.posterior[(0, 1, 40)] == 0.5\n","\n","assert_equal(model.predict(np.array([[10, 20], [20, 60]])), np.array([1, 0]))\n","######################################################\n","X_clf = np.array([[1, 1], [1, -1], [-1,-1], [-1, 1]])\n","y_clf = np.array([1, 1, 0, 0])\n","\n","model = MyNaiveBayes(alpha=0.01).fit(X_clf, y_clf)\n","\n","assert np.abs(model.apriori[0] - 0.5) < 0.01\n","assert np.abs(model.apriori[1] - 0.5) < 0.01\n","assert model.posterior[(0,1,1)]== 0.5\n","assert model.posterior[(1,0,1)]== 1.0\n","\n","\n","assert_equal(model.predict(np.array([[1, 2], [-1, -2]])), np.array([1, 0]))\n","\n","######################################################\n","X_clf = np.array([[2], [2], [4]])\n","y_clf = np.array([0, 0, 1])\n","\n","model = MyNaiveBayes(alpha=0.01).fit(X_clf, y_clf)\n","\n","\n","assert np.abs(model.apriori[0] - 0.666) < 0.01\n","assert np.abs(model.apriori[1] - 0.333) < 0.01\n","assert model.posterior[(0,0,2)] == 1.0\n","assert model.posterior[(1,0,4)] == 1.0\n","\n","\n","\n","assert_equal(model.predict(np.array([[2], [4]])), np.array([0, 1]))\n","######################################################\n","X_clf = np.array([[4], [4], [4], [4], [2], [2], [2], [2], [2]])\n","y_clf = np.array([0, 0, 0, 0, 0, 0, 0, 1, 1])\n","\n","model = MyNaiveBayes(alpha=0.01).fit(X_clf, y_clf)\n","\n","#assert np.abs(model.apriori[0] - 0.777) < 0.01\n","assert np.abs(model.apriori[1] - 0.222) < 0.01\n","assert np.abs(model.posterior[(0,0,4)] - 0.5714) < 0.01\n","assert np.abs(model.posterior[(0,0,2)] - 0.4285) < 0.01\n","assert model.posterior[(1,0,2)] == 1.0\n","\n","assert_equal(model.predict(np.array([[2], [4]])), np.array([0, 0]))\n","######################################################\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score \n","from sklearn.naive_bayes import MultinomialNB\n","\n","random_state = 1448\n","X, y = make_classification(n_samples=500, n_features=4, n_informative=2,\n","                           scale=5, random_state=random_state)\n","\n","X = X.astype(np.int32) \n","X += np.abs(np.min(X))\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n","                                                    random_state=random_state,\n","                                                    stratify=y)\n","\n","clf = MyNaiveBayes(alpha=0.01).fit(X_train, y_train)\n","assert accuracy_score(y_test, clf.predict(X_test)) > 0.9\n","######################################################\n"]},{"cell_type":"markdown","metadata":{},"source":["# Злобные твиты"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Наивный Байес очень хорошо подходит для Анализа тональности текста. То есть определить несет текст позитивный или негативный вайб :) \n","\n","В данной задаче вам даны [твиты про разные авиакомпании](data) (файлы `tweets_train.csv`, `tweets_test.csv`). Требуется построить классификатор, который бы определял имеет ли твит положительную или негативную окраску. Напишите функцию `predict`, который возвращает массив, где 1 - это позитивный, 0 - негативный. \n","\n","В данной задаче вам понадобятся `TweetTokenizer` и `CountVectorizer`. \n","\n","* `TweetTokenizer` и любой tokenizer самостоятельно разбивает строку на слова как-то их модифицировав.\n","* `CountVectorizer` превращает предложение в вектор чисел основываясь на их частоте, используя токенайзер.\n","\n","Задача делится на 2 пункта: \n","\n","* Preprocessing: переведите все слова в нижний регистр, токенизируйте слова с помощью `TweetTokenizer`, а дальше опять соберите в предложение. Проделайте это и в $X_{train}$ и $X_{test}$\n","* Predict: переведите предложения в вектора количеств с помощь `CountVectorizer`. Обучите модель на данных и предскажите ответ.\n","\n","Чтобы получить баллы надо побить accuracy = 0.88\n","\n","\n","P.S Можете посмотреть топ слов, которые с наибольшей вероятностью относятся к позитивным и негативным классам. \n","\n","P.P.S. Если ничего не понятно: можете изучить различные kernel на [kaggle](https://www.kaggle.com/crowdflower/twitter-airline-sentiment)."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["from nltk.tokenize import WordPunctTokenizer, TweetTokenizer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","\n","def predict(df_train: pd.DataFrame, df_test: pd.DataFrame):\n","    tokenizer = TweetTokenizer()\n","    vectorizer = CountVectorizer(tokenizer=tokenizer.tokenize)\n","    \n","    X_train = vectorizer.fit_transform(df_train['text'].str.lower())\n","    y_train = (df_train['airline_sentiment'] == 'positive').astype(int)\n","    \n","    model = MultinomialNB()\n","    model.fit(X_train, y_train)\n","    \n","    X_test = vectorizer.transform(df_test['text'].str.lower())\n","    return model.predict(X_test)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["######################################################\n","from sklearn.metrics import accuracy_score\n","\n","df_train = pd.read_csv('data/tweets_train.csv')\n","df_test = pd.read_csv('data/tweets_test.csv')\n","\n","y_test = ((df_test['airline_sentiment'] == 'positive').astype(np.int64))\n","X_test = df_test.drop(columns='airline_sentiment')\n","\n","tokenizer = TweetTokenizer()\n","vectorizer = CountVectorizer(tokenizer=tokenizer.tokenize)\n","X_train = vectorizer.fit_transform(df_train['text'].str.lower())  \n","assert accuracy_score(y_test, predict(df_train, df_test)) > 0.88\n","######################################################"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  (0, 3215)\t1\n","  (0, 7431)\t1\n","  (0, 3589)\t1\n","  (0, 3259)\t1\n","  (0, 6300)\t1\n","  (0, 11788)\t1\n","  (0, 1432)\t2\n","  (0, 4889)\t1\n","  (0, 11499)\t1\n","  (0, 8912)\t1\n","  (0, 8178)\t1\n","  (0, 8623)\t1\n","  (0, 3951)\t1\n","  (0, 8899)\t1\n","  (0, 7509)\t1\n","  (0, 1834)\t1\n","  (0, 12232)\t1\n","  (0, 9481)\t1\n","  (0, 1428)\t1\n","  (0, 7732)\t1\n","  (0, 11950)\t1\n","  (0, 11282)\t1\n","  (0, 3617)\t1\n","  (0, 12257)\t1\n","  (0, 10749)\t1\n","  :\t:\n","  (9230, 7750)\t1\n","  (9230, 6117)\t1\n","  (9231, 7431)\t1\n","  (9231, 3259)\t1\n","  (9231, 8734)\t1\n","  (9231, 2679)\t1\n","  (9231, 6163)\t1\n","  (9231, 6237)\t1\n","  (9231, 12039)\t1\n","  (9231, 2951)\t1\n","  (9231, 6648)\t1\n","  (9231, 12050)\t1\n","  (9231, 8558)\t1\n","  (9231, 4337)\t1\n","  (9231, 5838)\t1\n","  (9231, 10271)\t1\n","  (9231, 5878)\t1\n","  (9231, 10675)\t1\n","  (9231, 8910)\t1\n","  (9231, 6620)\t1\n","  (9231, 4576)\t1\n","  (9231, 4881)\t1\n","  (9231, 11739)\t1\n","  (9231, 5625)\t1\n","  (9231, 3869)\t1\n"]}],"source":["print(X_train)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
